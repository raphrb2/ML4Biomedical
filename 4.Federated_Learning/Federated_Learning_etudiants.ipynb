{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":170741,"status":"ok","timestamp":1695859277931,"user":{"displayName":"Florent Jakubowski","userId":"16460074933049396717"},"user_tz":-120},"id":"ALHpQCeQMamo","outputId":"6195fafe-4b7f-4457-a395-b02b810737d8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://download.pytorch.org/whl/cu117\n","\u001b[31mERROR: Could not find a version that satisfies the requirement torch==2.0.0+cu117 (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for torch==2.0.0+cu117\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install torch==2.0.0+cu117 torchvision==0.15.1+cu117 torchaudio==2.0.1 --index-url https://download.pytorch.org/whl/cu117"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31636,"status":"ok","timestamp":1695859309553,"user":{"displayName":"Florent Jakubowski","userId":"16460074933049396717"},"user_tz":-120},"id":"6ld_3HmDr1z-","outputId":"0dedf726-725c-4c44-928c-ff611e44f22a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.1.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.42.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.1)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.23.5)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.2)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n"]}],"source":["!pip install matplotlib\n","!pip install numpy\n","!pip install pandas\n","!pip install scikit-learn"]},{"cell_type":"markdown","metadata":{"id":"v-dxfgsjDNJV"},"source":["## I. MNIST Data"]},{"cell_type":"markdown","metadata":{"id":"M7dz8HfgsPkT"},"source":["### Exercice 1 : Loading Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b6P1hr56G3Dz"},"outputs":[],"source":["import torch\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Subset\n","import random\n","\n","from copy import deepcopy\n","\n","import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"Q2z8JQpFxJLe"},"source":["1)Create a function iid_split. This function should take a dataset, nb_nodes, n_samples_per_node, batch_size, and shuffle as parameters. The goal is to divide the dataset into nb_nodes subsets (i.i.d.) and load each subset using PyTorch's DataLoader with the specified batch_size and shuffle, and then return a list of these DataLoaders.\n","\n","Steps:\n","Load Data: Use DataLoader to load n_samples_per_node from the dataset with shuffle.  \n","Split Data: Divide the loaded data into nb_nodes i.i.d subsets, create a DataLoader for each, and append it to a list.   \n","Return List: Return the list of DataLoaders created.   "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"41pRKxCrxI4_"},"outputs":[],"source":["def iid_split(dataset, nb_nodes, n_samples_per_node, batch_size, shuffle=True):\n","    data_loaders = []\n","    \n","    # Check if the dataset has enough samples\n","    if len(dataset) < nb_nodes * n_samples_per_node:\n","        raise ValueError(\"The dataset doesn't have enough samples\")\n","    \n","    # Choose n_samples_per_node * nb_nodes random indices from the dataset\n","    indices = random.sample(range(len(dataset)), nb_nodes * n_samples_per_node)\n","    \n","    # Divide the selected indices into nb_nodes subsets\n","    for i in range(nb_nodes):\n","        start_idx = i * n_samples_per_node\n","        end_idx = (i + 1) * n_samples_per_node\n","        subset_indices = indices[start_idx:end_idx]\n","        \n","        # Create a Subset of the dataset using the selected indices and then a DataLoader\n","        subset = Subset(dataset, subset_indices)\n","        loader = DataLoader(subset, batch_size=batch_size, shuffle=shuffle)\n","        data_loaders.append(loader)\n","    \n","    return data_loaders\n"]},{"cell_type":"markdown","metadata":{"id":"rFssgNjnyDN2"},"source":["2) Create a function non_iid_split, designed to divide a dataset non-i.i.d. It will receive parameters: dataset, nb_nodes, n_samples_per_node, batch_size, shuffle, and shuffle_digits.\n","\n","Steps:\n","Arrange Digits: Optionally shuffle digits and fairly split them among nb_nodes.  \n","Load Data: Utilize DataLoader to load nb_nodes*n_samples_per_node samples, considering the shuffle parameter.   \n","Split Data and Return: Create DataLoaders for each node containing samples with corresponding digits and append them to a list.   "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":140},"executionInfo":{"elapsed":31,"status":"error","timestamp":1695859311917,"user":{"displayName":"Florent Jakubowski","userId":"16460074933049396717"},"user_tz":-120},"id":"Fj8_IBmRxcyX","outputId":"8f2e0ba6-e875-44d2-f55f-80fa8728d27b"},"outputs":[{"ename":"SyntaxError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-35f1fe91a720>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    shuffle=shuffle)\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}],"source":["def non_iid_split(dataset, nb_nodes, n_samples_per_node, batch_size, shuffle, shuffle_digits=False):\n","    assert(nb_nodes>0 and nb_nodes<=10)\n","\n","    digits=torch.arange(10) if shuffle_digits==False else torch.randperm(10, generator=torch.Generator().manual_seed(0))\n","\n","    # split the digits in a fair way\n","    #### TO DO ####\n","\n","    # load and shuffle nb_nodes*n_samples_per_node from the dataset\n","    loader = torch.utils.data.DataLoader(dataset,\n","                                        batch_size=## TO DO ##,\n","                                        shuffle=shuffle)\n","    dataiter = iter(loader)\n","    images_train_mnist, labels_train_mnist = next(dataiter)\n","\n","    data_splitted=list()\n","    for i in range(nb_nodes):\n","        # get indices for the digits, i.e. a tensor of boolean values correponding to idx of elements to keep in images_train_mnist and labels_train_mnist\n","        ### TO DO ###\n","        # append data_splitted with current data loader\n","        ### TO DO ###\n","    # return data_splitted with all data loaders\n","    return data_splitted"]},{"cell_type":"markdown","metadata":{"id":"GjL3PBSjzzpn"},"source":["3) Create a function get_MNIST, that fetches the MNIST dataset and utilizes either iid_split or non_iid_split to return train and test DataLoaders. The parameters are type (\"iid\" or \"non_iid\"), n_samples_train, n_samples_test, n_clients, batch_size, and shuffle.\n","\n","Steps:\n","Load MNIST Dataset: Utilize the MNIST dataset from PyTorch datasets for both train and test.  \n","Apply Split Function: Depending on the type parameter, apply either iid_split or non_iid_split to the loaded datasets.   \n","Return DataLoaders: Return the created train and test DataLoaders lists.  \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Jcy4a_xzzQL"},"outputs":[],"source":["def get_MNIST(type=\"iid\", n_samples_train=200, n_samples_test=100, n_clients=3, batch_size=25, shuffle=True):\n","    pass"]},{"cell_type":"markdown","metadata":{"id":"34Kn3WUR0xev"},"source":["4) Use get_MNIST to get mnist_iid_train and mnist_iid_test dataloaders."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uJi1e0KvJzuh"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"6Ta793YC1Der"},"source":["5) Giving the following function plot_samples for iid data. Plot image samples of client 1, 2 and 3."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AqItEeb21LqZ"},"outputs":[],"source":["def plot_samples(data, channel:int, title=None, plot_name=\"\", n_examples =20):\n","\n","    n_rows = int(n_examples / 5)\n","    plt.figure(figsize=(1* n_rows, 1*n_rows))\n","    if title: plt.suptitle(title)\n","    X, y= data\n","    for idx in range(n_examples):\n","\n","        ax = plt.subplot(n_rows, 5, idx + 1)\n","\n","        image = 255 - X[idx, channel].view((28,28))\n","        ax.imshow(image, cmap='gist_gray')\n","        ax.axis(\"off\")\n","\n","    if plot_name!=\"\":plt.savefig(f\"plots/\"+plot_name+\".png\")\n","\n","    plt.tight_layout()"]},{"cell_type":"markdown","metadata":{"id":"AnWOE24T3s3K"},"source":["6) Plot samples this time for non_iid data for 3 clients."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9lLoRF_k3sSL"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"kMLHXjzz2YMk"},"source":["### Exercice 2 : FedAvg and FedProx Implementation"]},{"cell_type":"markdown","metadata":{"id":"Awh8GJr52gf_"},"source":["Here is a simple CNN."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8hl-nO2JGwiD"},"outputs":[],"source":["class CNN(nn.Module):\n","\n","    \"\"\"ConvNet -> Max_Pool -> RELU -> ConvNet ->\n","    Max_Pool -> RELU -> FC -> RELU -> FC -> SOFTMAX\"\"\"\n","    def __init__(self):\n","        super(CNN, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n","        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n","        self.fc1 = nn.Linear(4*4*50, 500)\n","        self.fc2 = nn.Linear(500, 10)\n","\n","    def forward(self, x):\n","        x = F.relu(self.conv1(x))\n","        x = F.max_pool2d(x, 2, 2)\n","        x = F.relu(self.conv2(x))\n","        x = F.max_pool2d(x, 2, 2)\n","        x = x.view(-1, 4*4*50)\n","        x = F.relu(self.fc1(x))\n","        x = self.fc2(x)\n","\n","        return x\n","\n","model_0 = CNN()"]},{"cell_type":"markdown","metadata":{"id":"tqrpiwn14N3F"},"source":["1) Create a function difference_models_norm_2(model_1, model_2) which take two models, get models parameters and returns the sum of the square differences of models parameters.\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q-qTUlG85261"},"outputs":[],"source":["def difference_models_norm_2(model_1, model_2):\n","    \"\"\"Return the norm 2 difference between the two model parameters\n","    \"\"\"\n","    pass"]},{"cell_type":"markdown","metadata":{"id":"UVAIDa8m5_Pz"},"source":["Here is the function to perform one epoch of training data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jduyyFu36BrC"},"outputs":[],"source":["def train_step(model, model_0, mu:int, optimizer, train_data, loss_f):\n","    \"\"\"Train `model` on one epoch of `train_data`\"\"\"\n","\n","    total_loss=0\n","\n","    for idx, (features,labels) in enumerate(train_data):\n","\n","        optimizer.zero_grad()\n","\n","        predictions= model(features)\n","\n","        loss=loss_f(predictions,labels)\n","        loss+=mu/2*difference_models_norm_2(model,model_0)\n","        total_loss+=loss\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","    return total_loss/(idx+1)"]},{"cell_type":"markdown","metadata":{"id":"VYNLNhJI6ZXG"},"source":["2) Create a function local_learning which perform a local training of a model sent. We need to precise number of epochs needed to be performed locally. Use previous function defined.\n","\n","Return the local_loss compute during local training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mLNceMNa6bFL"},"outputs":[],"source":["def local_learning(model, mu:float, optimizer, train_data, epochs:int, loss_f):\n","\n","    ### Copy model to a new variable ###\n","    ### TO DO ###\n","\n","    for epoch in range(epochs):\n","        local_loss=### TO DO ###\n","\n","    return float(local_loss.detach().numpy())"]},{"cell_type":"markdown","metadata":{"id":"yf89aLlG7n8L"},"source":["We defined 4 others usefull functions :\n","- loss_classifier :\n","- loss_dataset :\n","- accuracy_dataset :\n","- set_to_zero_model_weights :\n","\n","Take time to read it."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"68VGErzD7-7-"},"outputs":[],"source":["def loss_classifier(predictions,labels):\n","\n","    m = nn.LogSoftmax(dim=1)\n","    loss = nn.NLLLoss(reduction=\"mean\")\n","\n","    return loss(m(predictions) ,labels.view(-1))\n","\n","\n","def loss_dataset(model, dataset, loss_f):\n","    \"\"\"Compute the loss of `model` on `dataset`\"\"\"\n","    loss=0\n","\n","    for idx,(features,labels) in enumerate(dataset):\n","\n","        predictions= model(features)\n","        loss+=loss_f(predictions,labels)\n","\n","    loss/=idx+1\n","    return loss\n","\n","\n","def accuracy_dataset(model, dataset):\n","    \"\"\"Compute the accuracy of `model` on `dataset`\"\"\"\n","\n","    correct=0\n","\n","    for features,labels in iter(dataset):\n","\n","        predictions= model(features)\n","\n","        _,predicted=predictions.max(1,keepdim=True)\n","\n","        correct+=torch.sum(predicted.view(-1,1)==labels.view(-1, 1)).item()\n","\n","    accuracy = 100*correct/len(dataset.dataset)\n","\n","    return accuracy\n","\n","def set_to_zero_model_weights(model):\n","    \"\"\"Set all the parameters of a model to 0\"\"\"\n","\n","    for layer_weigths in model.parameters():\n","        layer_weigths.data.sub_(layer_weigths.data)"]},{"cell_type":"markdown","metadata":{"id":"qy4MEJU08K05"},"source":["3) Defined a function average_models, which as it is written average parameters of a list of models to create a new model.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zUT09s28HCZm"},"outputs":[],"source":["def average_models(model, clients_models_hist:list , weights:list):\n","\n","\n","    \"\"\"Creates the new model of a given iteration with the models of the other\n","    clients\"\"\"\n","\n","    new_model=deepcopy(model)\n","    set_to_zero_model_weights(new_model)\n","\n","    for k,client_hist in enumerate(clients_models_hist):\n","\n","        for idx, layer_weights in enumerate(new_model.parameters()):\n","\n","            contribution=client_hist[idx].data*weights[k]\n","            layer_weights.data.add_(contribution)\n","\n","    return new_model"]},{"cell_type":"markdown","metadata":{"id":"7hIGhpAL9aWu"},"source":["4) We will now code a function to perform FedProx."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kxSLDqvyHHat"},"outputs":[],"source":["def FedProx(model, training_sets:list, n_iter:int, testing_sets:list, mu=0,\n","    file_name=\"test\", epochs=5, lr=10**-2, decay=1):\n","    \"\"\" all the clients are considered in this implementation of FedProx\n","    Parameters:\n","        - `model`: common structure used by the clients and the server\n","        - `training_sets`: list of the training sets. At each index is the\n","            training set of client \"index\"\n","        - `n_iter`: number of iterations the server will run\n","        - `testing_set`: list of the testing sets. If [], then the testing\n","            accuracy is not computed\n","        - `mu`: regularization term for FedProx. mu=0 for FedAvg\n","        - `epochs`: number of epochs each client is running\n","        - `lr`: learning rate of the optimizer\n","        - `decay`: to change the learning rate at each iteration\n","\n","    returns :\n","        - `model`: the final global model\n","    \"\"\"\n","\n","    loss_f=loss_classifier\n","\n","    #Variables initialization\n","    K=len(training_sets) #number of clients\n","    n_samples=sum([len(db.dataset) for db in training_sets])\n","    weights=([len(db.dataset)/n_samples for db in training_sets])\n","    print(\"Clients' weights:\",weights)\n","\n","\n","    loss_hist=[[float(loss_dataset(model, dl, loss_f).detach())\n","        for dl in training_sets]]\n","    acc_hist=[[accuracy_dataset(model, dl) for dl in testing_sets]]\n","    server_hist=[[tens_param.detach().numpy()\n","        for tens_param in list(model.parameters())]]\n","    models_hist = []\n","\n","\n","    server_loss=sum([weights[i]*loss_hist[-1][i] for i in range(len(weights))])\n","    server_acc=sum([weights[i]*acc_hist[-1][i] for i in range(len(weights))])\n","    print(f'====> i: 0 Loss: {server_loss} Server Test Accuracy: {server_acc}')\n","\n","    for i in range(n_iter):\n","\n","        clients_params=[]\n","        clients_models=[]\n","        clients_losses=[]\n","\n","        for k in range(K):\n","\n","            local_model=### TO DO ###\n","            # Define optimizer for local_model, don't forget the learning rate !\n","            local_optimizer=### TO DO ###\n","            # compute local_loss by performing learning steps on the current model\n","            local_loss= ### TO DO ###\n","\n","            clients_losses.append(local_loss)\n","\n","            #GET THE PARAMETER TENSORS OF THE MODEL\n","            list_params=list(local_model.parameters())\n","            list_params=[tens_param.detach() for tens_param in list_params]\n","            clients_params.append(list_params)\n","            clients_models.append(deepcopy(local_model))\n","\n","\n","        #CREATE THE NEW GLOBAL MODEL\n","        # Create new global model by avering all locals models\n","        model = ### To do ###\n","        models_hist.append(clients_models)\n","\n","        #COMPUTE THE LOSS/ACCURACY OF THE DIFFERENT CLIENTS WITH THE NEW MODEL\n","        loss_hist+=[[float(loss_dataset(model, dl, loss_f).detach())\n","            for dl in training_sets]]\n","        acc_hist+=[[accuracy_dataset(model, dl) for dl in testing_sets]]\n","\n","        server_loss=sum([weights[i]*loss_hist[-1][i] for i in range(len(weights))])\n","        server_acc=sum([weights[i]*acc_hist[-1][i] for i in range(len(weights))])\n","\n","        print(f'====> i: {i+1} Loss: {server_loss} Server Test Accuracy: {server_acc}')\n","\n","\n","        server_hist.append([tens_param.detach().cpu().numpy()\n","            for tens_param in list(model.parameters())])\n","\n","        #DECREASING THE LEARNING RATE AT EACH SERVER ITERATION\n","        lr*=decay\n","\n","    return model, loss_hist, acc_hist"]},{"cell_type":"markdown","metadata":{"id":"1NJ0VrGOHLMh"},"source":["###  Exercice 3 : iid data"]},{"cell_type":"markdown","metadata":{"id":"tYVeXiobBvRV"},"source":["#### Fed training with FedAvg"]},{"cell_type":"markdown","metadata":{"id":"qURGwRNXAzjQ"},"source":["1) With FedProx compute a FedAvg on iid data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rcuFJya6HP7U"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"PxECdf8cBItv"},"source":["2) Define a function plot_accuracy_loss to plot accuracy and loss of performed FedAvg for the 3 different clients."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gr56Yu3rBhny"},"outputs":[],"source":["def plot_acc_loss(title:str, loss_hist:list, acc_hist:list):\n","  pass"]},{"cell_type":"markdown","metadata":{"id":"lYf1MbP2HlWD"},"source":["#### FedPRox"]},{"cell_type":"markdown","metadata":{"id":"-6kik1CoB9C1"},"source":["3) Do the same as for 1) and 2).\n","You can take 2 local epochs, a learning rate of 0.1, mu = 0.3 and 10 iterations."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l8qBIDCZHnHB"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qy2a8uMWHt0u"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"DAxLyz7yKU47"},"source":["4) What do you observe ?"]},{"cell_type":"markdown","metadata":{"id":"PbClZcITMbkk"},"source":[]},{"cell_type":"markdown","metadata":{"id":"LH0AzCoOH3Lh"},"source":["### Exercice 4 : Non iid data"]},{"cell_type":"markdown","metadata":{"id":"J50kEMPUCLXl"},"source":["1) Perform a FedAvg on data and plot accuracy and loss\n","2) Do the same for a FedProx."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jd9nHSdjH8S2"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"60KikAGDH-m0"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RYtw0QuYIUEm"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z9aQF5oqIYyx"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u5eBgOX5IjxG"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P6JHGfbiIq47"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"vnm-Y1ssKYHP"},"source":["2) What do you observe ?"]},{"cell_type":"markdown","metadata":{"id":"YbokIHOLMdJS"},"source":[]},{"cell_type":"markdown","metadata":{"id":"HvSvunf4ybED"},"source":["## II. Medical Data"]},{"cell_type":"markdown","metadata":{"id":"7leu2_UgDa6G"},"source":["### Exercice 5 : medmnist"]},{"cell_type":"markdown","metadata":{"id":"M1B9G05NyCcG"},"source":["For medical datasets we will use medmnist package.  \n","Medmnist is large-scale MNIST-like collection of standardized biomedical images, including 12 datasets for 2D and 6 datasets for 3D : https://medmnist.com/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xh3lsOPIaTFG"},"outputs":[],"source":["! pip install medmnist\n"]},{"cell_type":"markdown","metadata":{"id":"Ha2lt3iFyfPD"},"source":["Import medmnist"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MXoi95WManMi"},"outputs":[],"source":["import medmnist"]},{"cell_type":"markdown","metadata":{"id":"M-CLcn6Gyj5r"},"source":["You can list all available datasets with this command"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IU9V12wxau2U"},"outputs":[],"source":["!python -m medmnist available"]},{"cell_type":"markdown","metadata":{"id":"uJJAUxpZyq9J"},"source":["We will use BloodMNIST for the following exercices. BloodMnist is a dataset with 8 categories of blood cells capture by microscope."]},{"cell_type":"markdown","metadata":{"id":"GuzPzm2kzSmU"},"source":["1. Import from medmnsit BloodMNIST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"moFXevwia_GJ"},"outputs":[],"source":["from medmnist import BloodMNIST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IT2KI08ddjRS"},"outputs":[],"source":["from tqdm import tqdm\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.utils.data as data\n","import torchvision.transforms as transforms\n","\n","import medmnist\n","from medmnist import INFO, Evaluator"]},{"cell_type":"markdown","metadata":{"id":"H-Gay0IKzlFb"},"source":["2. Create a preprocessing pipeline for data, transforms it into Tensor and then normalize it with a mean of 0.5 and standard deviation of 0.5 also."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SlOKfzEbzjuk"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"-kBeKkaGz_8Q"},"source":["3. Create a train dataset and test dataset from data and apply previous preprocessing pipeline. You can use DataClass from pytorch to create datasets."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ILobNOH5dynG"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"nkWOFiCQ04XJ"},"source":["4. Just to understand data print the 2 datasets to see differences with previous MNIST data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-cIYr2MzshnZ"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"wRJ1ndDJKole"},"source":["5. Create a data loader for train data and test dat, you can use a batch size of 128 and shuffle data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eVfI7YIZ04H9"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"i33g8uOl1V8Q"},"source":["6. Modify previous functions non_iid_split, iid_split, get_MNIST and plot_samples to work with new data.  \n","a. non_iid_split : this time there are only 8 classes   \n","b. iid_split : nothing really changed  \n","c. get_MNIST : Like in 3. use train and test datasets with the preprocessing pipeline we defined previously.  \n","d. plot_samples : Make it work for this new images. Also try to print to which classes belong every images, to see later if your iid_split and non_iid_split work well."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"48gVTHQyC7cs"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5VdEwGiCLEmz"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k6CVY7NqLFic"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"0PQACmFlMiUL"},"source":["### Exercice 6 : Use federated learning on medical data"]},{"cell_type":"markdown","metadata":{"id":"BALUn7umik0t"},"source":["### IID data"]},{"cell_type":"markdown","metadata":{"id":"A9g9Edac1pHO"},"source":["1. Load train data and test data for 3 clients into IID datasets."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"loSiYTVfC7cs"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"ynnjgo0R26h9"},"source":["2. plost samples with corresponding function."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KzWBwDtOC7ct"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"BLovf_tp4BPL"},"source":["3. Design a convlution neural network to perform classification.  \n","You can try to adpat previous convolution network.   \n","Or also you can try this type of architecture :  \n","- A first 2D convolution layer with an output_channels of 16, a BatchNormalisation and a Relu activation   \n","- A second 2D conv layer identical but with a MaxPooling layer after  \n","- A 3rd conv layer with an output_channels of 64, a BatchNormalisation and a Relu activation again.\n","- A 4rd layer identical to 3rd one.  \n","- A 5rd layer identical but with a MaxPooling added. Don't forget to use padding to not loose informations from corners.\n","- Finally add dense layers to perform classification. You can add regularization layers between dropout for example.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n2iey_qPC7ct"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xj7wDhAT1_-r"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"9zxI4gdAC7cu"},"source":["### Fed training with FedAvg"]},{"cell_type":"markdown","metadata":{"id":"Ou50J8N5Le5W"},"source":["4. Perform a FedAvg with same parameters as for MNIST datasets. Use 3 clients, etc. Try to run more iterations."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fMnCTdoPC7cu"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"25rt8KmMC7cv"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"PRzBpTLwC7cv"},"source":["### FedPRox"]},{"cell_type":"markdown","metadata":{"id":"Pt2Nkqt2Lve2"},"source":["5. As FedAvg perform a FedProx on iid data. Keep the same Âµ."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qjgZCAhGC7cv"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0U9Wx0YHC7cv"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"T7Qhr8c8C7cv"},"source":["### MNIST Non iid"]},{"cell_type":"markdown","metadata":{"id":"DzZ8qJYEL49j"},"source":["6. Do the same process (FedAVG, FedProx, plot loss and accuracy) but this time on non iid data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CdcJ9Z5SC7cw"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sUizEj6SC7cw"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8bfPfHDgC7cw"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RuzECkMDC7cw"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TReVWSFpC7cx"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Jg-OicjC7cx"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"IvQ-4vYNq3pv"},"source":["7. Is results satisfying ? Submit some way to improve results and resolve possible unstabilities. For this you can check original paper : https://arxiv.org/pdf/1812.06127.pdf"]},{"cell_type":"markdown","metadata":{"id":"pq6OAZjBLXb6"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tnw4bnmYrdjT"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOWBDnFaySwA16b5nGUTiXI","collapsed_sections":["iqr9-UE9uTzT"],"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":0}
