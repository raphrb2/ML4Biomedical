{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["iqr9-UE9uTzT"],"toc_visible":true,"authorship_tag":"ABX9TyOWBDnFaySwA16b5nGUTiXI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install torch==2.0.0+cu117 torchvision==0.15.1+cu117 torchaudio==2.0.1 --index-url https://download.pytorch.org/whl/cu117"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ALHpQCeQMamo","executionInfo":{"status":"ok","timestamp":1695859277931,"user_tz":-120,"elapsed":170741,"user":{"displayName":"Florent Jakubowski","userId":"16460074933049396717"}},"outputId":"6195fafe-4b7f-4457-a395-b02b810737d8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://download.pytorch.org/whl/cu117\n","Collecting torch==2.0.0+cu117\n","  Downloading https://download.pytorch.org/whl/cu117/torch-2.0.0%2Bcu117-cp310-cp310-linux_x86_64.whl (1843.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 GB\u001b[0m \u001b[31m570.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torchvision==0.15.1+cu117\n","  Downloading https://download.pytorch.org/whl/cu117/torchvision-0.15.1%2Bcu117-cp310-cp310-linux_x86_64.whl (6.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torchaudio==2.0.1\n","  Downloading https://download.pytorch.org/whl/cu117/torchaudio-2.0.1%2Bcu117-cp310-cp310-linux_x86_64.whl (4.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0+cu117) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0+cu117) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0+cu117) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0+cu117) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0+cu117) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0+cu117) (2.0.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.1+cu117) (1.23.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.1+cu117) (2.31.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.1+cu117) (9.4.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0+cu117) (3.27.4.1)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0+cu117) (16.0.6)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.0+cu117) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.1+cu117) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.1+cu117) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.1+cu117) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.1+cu117) (2023.7.22)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.0+cu117) (1.3.0)\n","Installing collected packages: torch, torchvision, torchaudio\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.0.1+cu118\n","    Uninstalling torch-2.0.1+cu118:\n","      Successfully uninstalled torch-2.0.1+cu118\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.15.2+cu118\n","    Uninstalling torchvision-0.15.2+cu118:\n","      Successfully uninstalled torchvision-0.15.2+cu118\n","  Attempting uninstall: torchaudio\n","    Found existing installation: torchaudio 2.0.2+cu118\n","    Uninstalling torchaudio-2.0.2+cu118:\n","      Successfully uninstalled torchaudio-2.0.2+cu118\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchdata 0.6.1 requires torch==2.0.1, but you have torch 2.0.0+cu117 which is incompatible.\n","torchtext 0.15.2 requires torch==2.0.1, but you have torch 2.0.0+cu117 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed torch-2.0.0+cu117 torchaudio-2.0.1+cu117 torchvision-0.15.1+cu117\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6ld_3HmDr1z-","executionInfo":{"status":"ok","timestamp":1695859309553,"user_tz":-120,"elapsed":31636,"user":{"displayName":"Florent Jakubowski","userId":"16460074933049396717"}},"outputId":"0dedf726-725c-4c44-928c-ff611e44f22a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.1.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.42.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.1)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.23.5)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.2)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n"]}],"source":["!pip install matplotlib\n","!pip install numpy\n","!pip install pandas\n","!pip install scikit-learn"]},{"cell_type":"markdown","source":["## I. MNIST Data"],"metadata":{"id":"v-dxfgsjDNJV"}},{"cell_type":"markdown","source":["### Exercice 1 : Loading Data"],"metadata":{"id":"M7dz8HfgsPkT"}},{"cell_type":"code","source":["import torch\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","from copy import deepcopy\n","\n","import numpy as np\n","import matplotlib.pyplot as plt"],"metadata":{"id":"b6P1hr56G3Dz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["1)Create a function iid_split. This function should take a dataset, nb_nodes, n_samples_per_node, batch_size, and shuffle as parameters. The goal is to divide the dataset into nb_nodes subsets (i.i.d.) and load each subset using PyTorch's DataLoader with the specified batch_size and shuffle, and then return a list of these DataLoaders.\n","\n","Steps:\n","Load Data: Use DataLoader to load n_samples_per_node from the dataset with shuffle.  \n","Split Data: Divide the loaded data into nb_nodes i.i.d subsets, create a DataLoader for each, and append it to a list.   \n","Return List: Return the list of DataLoaders created.   "],"metadata":{"id":"Q2z8JQpFxJLe"}},{"cell_type":"code","source":["def iid_split(dataset, nb_nodes, n_samples_per_node, batch_size, shuffle):\n","  pass"],"metadata":{"id":"41pRKxCrxI4_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2) Create a function non_iid_split, designed to divide a dataset non-i.i.d. It will receive parameters: dataset, nb_nodes, n_samples_per_node, batch_size, shuffle, and shuffle_digits.\n","\n","Steps:\n","Arrange Digits: Optionally shuffle digits and fairly split them among nb_nodes.  \n","Load Data: Utilize DataLoader to load nb_nodes*n_samples_per_node samples, considering the shuffle parameter.   \n","Split Data and Return: Create DataLoaders for each node containing samples with corresponding digits and append them to a list.   "],"metadata":{"id":"rFssgNjnyDN2"}},{"cell_type":"code","source":["def non_iid_split(dataset, nb_nodes, n_samples_per_node, batch_size, shuffle, shuffle_digits=False):\n","    assert(nb_nodes>0 and nb_nodes<=10)\n","\n","    digits=torch.arange(10) if shuffle_digits==False else torch.randperm(10, generator=torch.Generator().manual_seed(0))\n","\n","    # split the digits in a fair way\n","    #### TO DO ####\n","\n","    # load and shuffle nb_nodes*n_samples_per_node from the dataset\n","    loader = torch.utils.data.DataLoader(dataset,\n","                                        batch_size=## TO DO ##,\n","                                        shuffle=shuffle)\n","    dataiter = iter(loader)\n","    images_train_mnist, labels_train_mnist = next(dataiter)\n","\n","    data_splitted=list()\n","    for i in range(nb_nodes):\n","        # get indices for the digits, i.e. a tensor of boolean values correponding to idx of elements to keep in images_train_mnist and labels_train_mnist\n","        ### TO DO ###\n","        # append data_splitted with current data loader\n","        ### TO DO ###\n","    # return data_splitted with all data loaders\n","    return data_splitted"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":140},"id":"Fj8_IBmRxcyX","executionInfo":{"status":"error","timestamp":1695859311917,"user_tz":-120,"elapsed":31,"user":{"displayName":"Florent Jakubowski","userId":"16460074933049396717"}},"outputId":"8f2e0ba6-e875-44d2-f55f-80fa8728d27b"},"execution_count":null,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-35f1fe91a720>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    shuffle=shuffle)\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"cell_type":"markdown","source":["3) Create a function get_MNIST, that fetches the MNIST dataset and utilizes either iid_split or non_iid_split to return train and test DataLoaders. The parameters are type (\"iid\" or \"non_iid\"), n_samples_train, n_samples_test, n_clients, batch_size, and shuffle.\n","\n","Steps:\n","Load MNIST Dataset: Utilize the MNIST dataset from PyTorch datasets for both train and test.  \n","Apply Split Function: Depending on the type parameter, apply either iid_split or non_iid_split to the loaded datasets.   \n","Return DataLoaders: Return the created train and test DataLoaders lists.  \n"],"metadata":{"id":"GjL3PBSjzzpn"}},{"cell_type":"code","source":["def get_MNIST(type=\"iid\", n_samples_train=200, n_samples_test=100, n_clients=3, batch_size=25, shuffle=True):\n","    pass"],"metadata":{"id":"3Jcy4a_xzzQL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["4) Use get_MNIST to get mnist_iid_train and mnist_iid_test dataloaders."],"metadata":{"id":"34Kn3WUR0xev"}},{"cell_type":"code","source":[],"metadata":{"id":"uJi1e0KvJzuh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["5) Giving the following function plot_samples for iid data. Plot image samples of client 1, 2 and 3."],"metadata":{"id":"6Ta793YC1Der"}},{"cell_type":"code","source":["def plot_samples(data, channel:int, title=None, plot_name=\"\", n_examples =20):\n","\n","    n_rows = int(n_examples / 5)\n","    plt.figure(figsize=(1* n_rows, 1*n_rows))\n","    if title: plt.suptitle(title)\n","    X, y= data\n","    for idx in range(n_examples):\n","\n","        ax = plt.subplot(n_rows, 5, idx + 1)\n","\n","        image = 255 - X[idx, channel].view((28,28))\n","        ax.imshow(image, cmap='gist_gray')\n","        ax.axis(\"off\")\n","\n","    if plot_name!=\"\":plt.savefig(f\"plots/\"+plot_name+\".png\")\n","\n","    plt.tight_layout()"],"metadata":{"id":"AqItEeb21LqZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["6) Plot samples this time for non_iid data for 3 clients."],"metadata":{"id":"AnWOE24T3s3K"}},{"cell_type":"code","source":[],"metadata":{"id":"9lLoRF_k3sSL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Exercice 2 : FedAvg and FedProx Implementation"],"metadata":{"id":"kMLHXjzz2YMk"}},{"cell_type":"markdown","source":["Here is a simple CNN."],"metadata":{"id":"Awh8GJr52gf_"}},{"cell_type":"code","source":["class CNN(nn.Module):\n","\n","    \"\"\"ConvNet -> Max_Pool -> RELU -> ConvNet ->\n","    Max_Pool -> RELU -> FC -> RELU -> FC -> SOFTMAX\"\"\"\n","    def __init__(self):\n","        super(CNN, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n","        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n","        self.fc1 = nn.Linear(4*4*50, 500)\n","        self.fc2 = nn.Linear(500, 10)\n","\n","    def forward(self, x):\n","        x = F.relu(self.conv1(x))\n","        x = F.max_pool2d(x, 2, 2)\n","        x = F.relu(self.conv2(x))\n","        x = F.max_pool2d(x, 2, 2)\n","        x = x.view(-1, 4*4*50)\n","        x = F.relu(self.fc1(x))\n","        x = self.fc2(x)\n","\n","        return x\n","\n","model_0 = CNN()"],"metadata":{"id":"8hl-nO2JGwiD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["1) Create a function difference_models_norm_2(model_1, model_2) which take two models, get models parameters and returns the sum of the square differences of models parameters.\n","\n","\n","\n"],"metadata":{"id":"tqrpiwn14N3F"}},{"cell_type":"code","source":["def difference_models_norm_2(model_1, model_2):\n","    \"\"\"Return the norm 2 difference between the two model parameters\n","    \"\"\"\n","    pass"],"metadata":{"id":"Q-qTUlG85261"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here is the function to perform one epoch of training data."],"metadata":{"id":"UVAIDa8m5_Pz"}},{"cell_type":"code","source":["def train_step(model, model_0, mu:int, optimizer, train_data, loss_f):\n","    \"\"\"Train `model` on one epoch of `train_data`\"\"\"\n","\n","    total_loss=0\n","\n","    for idx, (features,labels) in enumerate(train_data):\n","\n","        optimizer.zero_grad()\n","\n","        predictions= model(features)\n","\n","        loss=loss_f(predictions,labels)\n","        loss+=mu/2*difference_models_norm_2(model,model_0)\n","        total_loss+=loss\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","    return total_loss/(idx+1)"],"metadata":{"id":"jduyyFu36BrC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2) Create a function local_learning which perform a local training of a model sent. We need to precise number of epochs needed to be performed locally. Use previous function defined.\n","\n","Return the local_loss compute during local training."],"metadata":{"id":"VYNLNhJI6ZXG"}},{"cell_type":"code","source":["def local_learning(model, mu:float, optimizer, train_data, epochs:int, loss_f):\n","\n","    ### Copy model to a new variable ###\n","    ### TO DO ###\n","\n","    for epoch in range(epochs):\n","        local_loss=### TO DO ###\n","\n","    return float(local_loss.detach().numpy())"],"metadata":{"id":"mLNceMNa6bFL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We defined 4 others usefull functions :\n","- loss_classifier :\n","- loss_dataset :\n","- accuracy_dataset :\n","- set_to_zero_model_weights :\n","\n","Take time to read it."],"metadata":{"id":"yf89aLlG7n8L"}},{"cell_type":"code","source":["def loss_classifier(predictions,labels):\n","\n","    m = nn.LogSoftmax(dim=1)\n","    loss = nn.NLLLoss(reduction=\"mean\")\n","\n","    return loss(m(predictions) ,labels.view(-1))\n","\n","\n","def loss_dataset(model, dataset, loss_f):\n","    \"\"\"Compute the loss of `model` on `dataset`\"\"\"\n","    loss=0\n","\n","    for idx,(features,labels) in enumerate(dataset):\n","\n","        predictions= model(features)\n","        loss+=loss_f(predictions,labels)\n","\n","    loss/=idx+1\n","    return loss\n","\n","\n","def accuracy_dataset(model, dataset):\n","    \"\"\"Compute the accuracy of `model` on `dataset`\"\"\"\n","\n","    correct=0\n","\n","    for features,labels in iter(dataset):\n","\n","        predictions= model(features)\n","\n","        _,predicted=predictions.max(1,keepdim=True)\n","\n","        correct+=torch.sum(predicted.view(-1,1)==labels.view(-1, 1)).item()\n","\n","    accuracy = 100*correct/len(dataset.dataset)\n","\n","    return accuracy\n","\n","def set_to_zero_model_weights(model):\n","    \"\"\"Set all the parameters of a model to 0\"\"\"\n","\n","    for layer_weigths in model.parameters():\n","        layer_weigths.data.sub_(layer_weigths.data)"],"metadata":{"id":"68VGErzD7-7-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3) Defined a function average_models, which as it is written average parameters of a list of models to create a new model.\n"],"metadata":{"id":"qy4MEJU08K05"}},{"cell_type":"code","source":["def average_models(model, clients_models_hist:list , weights:list):\n","\n","\n","    \"\"\"Creates the new model of a given iteration with the models of the other\n","    clients\"\"\"\n","\n","    new_model=deepcopy(model)\n","    set_to_zero_model_weights(new_model)\n","\n","    for k,client_hist in enumerate(clients_models_hist):\n","\n","        for idx, layer_weights in enumerate(new_model.parameters()):\n","\n","            contribution=client_hist[idx].data*weights[k]\n","            layer_weights.data.add_(contribution)\n","\n","    return new_model"],"metadata":{"id":"zUT09s28HCZm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["4) We will now code a function to perform FedProx."],"metadata":{"id":"7hIGhpAL9aWu"}},{"cell_type":"code","source":["def FedProx(model, training_sets:list, n_iter:int, testing_sets:list, mu=0,\n","    file_name=\"test\", epochs=5, lr=10**-2, decay=1):\n","    \"\"\" all the clients are considered in this implementation of FedProx\n","    Parameters:\n","        - `model`: common structure used by the clients and the server\n","        - `training_sets`: list of the training sets. At each index is the\n","            training set of client \"index\"\n","        - `n_iter`: number of iterations the server will run\n","        - `testing_set`: list of the testing sets. If [], then the testing\n","            accuracy is not computed\n","        - `mu`: regularization term for FedProx. mu=0 for FedAvg\n","        - `epochs`: number of epochs each client is running\n","        - `lr`: learning rate of the optimizer\n","        - `decay`: to change the learning rate at each iteration\n","\n","    returns :\n","        - `model`: the final global model\n","    \"\"\"\n","\n","    loss_f=loss_classifier\n","\n","    #Variables initialization\n","    K=len(training_sets) #number of clients\n","    n_samples=sum([len(db.dataset) for db in training_sets])\n","    weights=([len(db.dataset)/n_samples for db in training_sets])\n","    print(\"Clients' weights:\",weights)\n","\n","\n","    loss_hist=[[float(loss_dataset(model, dl, loss_f).detach())\n","        for dl in training_sets]]\n","    acc_hist=[[accuracy_dataset(model, dl) for dl in testing_sets]]\n","    server_hist=[[tens_param.detach().numpy()\n","        for tens_param in list(model.parameters())]]\n","    models_hist = []\n","\n","\n","    server_loss=sum([weights[i]*loss_hist[-1][i] for i in range(len(weights))])\n","    server_acc=sum([weights[i]*acc_hist[-1][i] for i in range(len(weights))])\n","    print(f'====> i: 0 Loss: {server_loss} Server Test Accuracy: {server_acc}')\n","\n","    for i in range(n_iter):\n","\n","        clients_params=[]\n","        clients_models=[]\n","        clients_losses=[]\n","\n","        for k in range(K):\n","\n","            local_model=### TO DO ###\n","            # Define optimizer for local_model, don't forget the learning rate !\n","            local_optimizer=### TO DO ###\n","            # compute local_loss by performing learning steps on the current model\n","            local_loss= ### TO DO ###\n","\n","            clients_losses.append(local_loss)\n","\n","            #GET THE PARAMETER TENSORS OF THE MODEL\n","            list_params=list(local_model.parameters())\n","            list_params=[tens_param.detach() for tens_param in list_params]\n","            clients_params.append(list_params)\n","            clients_models.append(deepcopy(local_model))\n","\n","\n","        #CREATE THE NEW GLOBAL MODEL\n","        # Create new global model by avering all locals models\n","        model = ### To do ###\n","        models_hist.append(clients_models)\n","\n","        #COMPUTE THE LOSS/ACCURACY OF THE DIFFERENT CLIENTS WITH THE NEW MODEL\n","        loss_hist+=[[float(loss_dataset(model, dl, loss_f).detach())\n","            for dl in training_sets]]\n","        acc_hist+=[[accuracy_dataset(model, dl) for dl in testing_sets]]\n","\n","        server_loss=sum([weights[i]*loss_hist[-1][i] for i in range(len(weights))])\n","        server_acc=sum([weights[i]*acc_hist[-1][i] for i in range(len(weights))])\n","\n","        print(f'====> i: {i+1} Loss: {server_loss} Server Test Accuracy: {server_acc}')\n","\n","\n","        server_hist.append([tens_param.detach().cpu().numpy()\n","            for tens_param in list(model.parameters())])\n","\n","        #DECREASING THE LEARNING RATE AT EACH SERVER ITERATION\n","        lr*=decay\n","\n","    return model, loss_hist, acc_hist"],"metadata":{"id":"kxSLDqvyHHat"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###  Exercice 3 : iid data"],"metadata":{"id":"1NJ0VrGOHLMh"}},{"cell_type":"markdown","source":["#### Fed training with FedAvg"],"metadata":{"id":"tYVeXiobBvRV"}},{"cell_type":"markdown","source":["1) With FedProx compute a FedAvg on iid data."],"metadata":{"id":"qURGwRNXAzjQ"}},{"cell_type":"code","source":[],"metadata":{"id":"rcuFJya6HP7U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2) Define a function plot_accuracy_loss to plot accuracy and loss of performed FedAvg for the 3 different clients."],"metadata":{"id":"PxECdf8cBItv"}},{"cell_type":"code","source":["def plot_acc_loss(title:str, loss_hist:list, acc_hist:list):\n","  pass"],"metadata":{"id":"gr56Yu3rBhny"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### FedPRox"],"metadata":{"id":"lYf1MbP2HlWD"}},{"cell_type":"markdown","source":["3) Do the same as for 1) and 2).\n","You can take 2 local epochs, a learning rate of 0.1, mu = 0.3 and 10 iterations."],"metadata":{"id":"-6kik1CoB9C1"}},{"cell_type":"code","source":[],"metadata":{"id":"l8qBIDCZHnHB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"qy2a8uMWHt0u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["4) What do you observe ?"],"metadata":{"id":"DAxLyz7yKU47"}},{"cell_type":"markdown","source":[],"metadata":{"id":"PbClZcITMbkk"}},{"cell_type":"markdown","source":["### Exercice 4 : Non iid data"],"metadata":{"id":"LH0AzCoOH3Lh"}},{"cell_type":"markdown","source":["1) Perform a FedAvg on data and plot accuracy and loss\n","2) Do the same for a FedProx."],"metadata":{"id":"J50kEMPUCLXl"}},{"cell_type":"code","source":[],"metadata":{"id":"jd9nHSdjH8S2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"60KikAGDH-m0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"RYtw0QuYIUEm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"z9aQF5oqIYyx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"u5eBgOX5IjxG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"P6JHGfbiIq47"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2) What do you observe ?"],"metadata":{"id":"vnm-Y1ssKYHP"}},{"cell_type":"markdown","source":[],"metadata":{"id":"YbokIHOLMdJS"}},{"cell_type":"markdown","source":["## II. Medical Data"],"metadata":{"id":"HvSvunf4ybED"}},{"cell_type":"markdown","source":["### Exercice 5 : medmnist"],"metadata":{"id":"7leu2_UgDa6G"}},{"cell_type":"markdown","source":["For medical datasets we will use medmnist package.  \n","Medmnist is large-scale MNIST-like collection of standardized biomedical images, including 12 datasets for 2D and 6 datasets for 3D : https://medmnist.com/"],"metadata":{"id":"M1B9G05NyCcG"}},{"cell_type":"code","source":["! pip install medmnist\n"],"metadata":{"id":"Xh3lsOPIaTFG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Import medmnist"],"metadata":{"id":"Ha2lt3iFyfPD"}},{"cell_type":"code","source":["import medmnist"],"metadata":{"id":"MXoi95WManMi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You can list all available datasets with this command"],"metadata":{"id":"M-CLcn6Gyj5r"}},{"cell_type":"code","source":["!python -m medmnist available"],"metadata":{"id":"IU9V12wxau2U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We will use BloodMNIST for the following exercices. BloodMnist is a dataset with 8 categories of blood cells capture by microscope."],"metadata":{"id":"uJJAUxpZyq9J"}},{"cell_type":"markdown","source":["1. Import from medmnsit BloodMNIST"],"metadata":{"id":"GuzPzm2kzSmU"}},{"cell_type":"code","source":["from medmnist import BloodMNIST"],"metadata":{"id":"moFXevwia_GJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.utils.data as data\n","import torchvision.transforms as transforms\n","\n","import medmnist\n","from medmnist import INFO, Evaluator"],"metadata":{"id":"IT2KI08ddjRS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2. Create a preprocessing pipeline for data, transforms it into Tensor and then normalize it with a mean of 0.5 and standard deviation of 0.5 also."],"metadata":{"id":"H-Gay0IKzlFb"}},{"cell_type":"code","source":[],"metadata":{"id":"SlOKfzEbzjuk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3. Create a train dataset and test dataset from data and apply previous preprocessing pipeline. You can use DataClass from pytorch to create datasets."],"metadata":{"id":"-kBeKkaGz_8Q"}},{"cell_type":"code","source":[],"metadata":{"id":"ILobNOH5dynG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["4. Just to understand data print the 2 datasets to see differences with previous MNIST data."],"metadata":{"id":"nkWOFiCQ04XJ"}},{"cell_type":"code","source":[],"metadata":{"id":"-cIYr2MzshnZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["5. Create a data loader for train data and test dat, you can use a batch size of 128 and shuffle data."],"metadata":{"id":"wRJ1ndDJKole"}},{"cell_type":"code","source":[],"metadata":{"id":"eVfI7YIZ04H9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["6. Modify previous functions non_iid_split, iid_split, get_MNIST and plot_samples to work with new data.  \n","a. non_iid_split : this time there are only 8 classes   \n","b. iid_split : nothing really changed  \n","c. get_MNIST : Like in 3. use train and test datasets with the preprocessing pipeline we defined previously.  \n","d. plot_samples : Make it work for this new images. Also try to print to which classes belong every images, to see later if your iid_split and non_iid_split work well."],"metadata":{"id":"i33g8uOl1V8Q"}},{"cell_type":"code","source":[],"metadata":{"id":"48gVTHQyC7cs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"5VdEwGiCLEmz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"k6CVY7NqLFic"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Exercice 6 : Use federated learning on medical data"],"metadata":{"id":"0PQACmFlMiUL"}},{"cell_type":"markdown","source":["### IID data"],"metadata":{"id":"BALUn7umik0t"}},{"cell_type":"markdown","source":["1. Load train data and test data for 3 clients into IID datasets."],"metadata":{"id":"A9g9Edac1pHO"}},{"cell_type":"code","source":[],"metadata":{"id":"loSiYTVfC7cs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2. plost samples with corresponding function."],"metadata":{"id":"ynnjgo0R26h9"}},{"cell_type":"code","source":[],"metadata":{"id":"KzWBwDtOC7ct"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3. Design a convlution neural network to perform classification.  \n","You can try to adpat previous convolution network.   \n","Or also you can try this type of architecture :  \n","- A first 2D convolution layer with an output_channels of 16, a BatchNormalisation and a Relu activation   \n","- A second 2D conv layer identical but with a MaxPooling layer after  \n","- A 3rd conv layer with an output_channels of 64, a BatchNormalisation and a Relu activation again.\n","- A 4rd layer identical to 3rd one.  \n","- A 5rd layer identical but with a MaxPooling added. Don't forget to use padding to not loose informations from corners.\n","- Finally add dense layers to perform classification. You can add regularization layers between dropout for example.\n","\n"],"metadata":{"id":"BLovf_tp4BPL"}},{"cell_type":"code","source":[],"metadata":{"id":"n2iey_qPC7ct"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"xj7wDhAT1_-r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Fed training with FedAvg"],"metadata":{"id":"9zxI4gdAC7cu"}},{"cell_type":"markdown","source":["4. Perform a FedAvg with same parameters as for MNIST datasets. Use 3 clients, etc. Try to run more iterations."],"metadata":{"id":"Ou50J8N5Le5W"}},{"cell_type":"code","source":[],"metadata":{"id":"fMnCTdoPC7cu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"25rt8KmMC7cv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### FedPRox"],"metadata":{"id":"PRzBpTLwC7cv"}},{"cell_type":"markdown","source":["5. As FedAvg perform a FedProx on iid data. Keep the same µ."],"metadata":{"id":"Pt2Nkqt2Lve2"}},{"cell_type":"code","source":[],"metadata":{"id":"qjgZCAhGC7cv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"0U9Wx0YHC7cv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### MNIST Non iid"],"metadata":{"id":"T7Qhr8c8C7cv"}},{"cell_type":"markdown","source":["6. Do the same process (FedAVG, FedProx, plot loss and accuracy) but this time on non iid data."],"metadata":{"id":"DzZ8qJYEL49j"}},{"cell_type":"code","source":[],"metadata":{"id":"CdcJ9Z5SC7cw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"sUizEj6SC7cw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"8bfPfHDgC7cw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"RuzECkMDC7cw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"TReVWSFpC7cx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"9Jg-OicjC7cx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["7. Is results satisfying ? Submit some way to improve results and resolve possible unstabilities. For this you can check original paper : https://arxiv.org/pdf/1812.06127.pdf"],"metadata":{"id":"IvQ-4vYNq3pv"}},{"cell_type":"markdown","source":[],"metadata":{"id":"pq6OAZjBLXb6"}},{"cell_type":"code","source":[],"metadata":{"id":"tnw4bnmYrdjT"},"execution_count":null,"outputs":[]}]}